---
title: ISA DeepData
date: "`r Sys.Date()`"
author: Pieter Provoost
output: (function(...) {
  rmdformats::robobook(toc_depth = 3, ...) })
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "docs") })  
---

This notebook processes data from the [International Seabed Authority](https://www.isa.org.jm/) (ISA) [DeepData](https://data.isa.org.jm/isa/map/) database into Darwin Core archives.

The code for this notebook is hosted at https://github.com/iobis/notebook-deepdata.

## Fetching the data from the ISA server

The DeepData dataset is delivered to OBIS via FTP. Let's list the files on the FTP server and download the most recent JSON file to a temporary directory. Credentials are stored in `env.txt`.

```{r message=FALSE, warning=FALSE}
require(RCurl)
library(stringr)
library(dplyr)

readRenviron("env.txt")
url <- Sys.getenv("deepdata_url")
res <- getURL(url, verbose = TRUE, ftp.use.epsv = FALSE, dirlistonly = TRUE)

filenames <- unlist(strsplit(res, "\\n"))
filenames <- sort(filenames[str_detect(filenames, ".json")], decreasing = TRUE)

if (length(filenames) == 0) {
  stop("No files found")
}

file_url <- paste0(url, "/", filenames[1])
temp_file <- tempfile(pattern = "deepdata_", tmpdir = tempdir(), fileext = ".json")
download.file(file_url, temp_file)
```

## Parsing the JSON file

As the JSON file is encoded in the non-standard `ISO-8859-1`, we need to use `readLines` before parsing the data with the jsonlite package.

```{r message=FALSE, warning=FALSE}
library(jsonlite)
library(purrr)

con <- file(temp_file)
lines <- readLines(file(temp_file, encoding = "ISO-8859-1"), warn = FALSE)
close(con)
records <- fromJSON(lines, simplifyDataFrame = TRUE)$DEEPDATA
```

## Generating Darwin Core data files

We can now extract a list of distinct datasets from the data frame, and generate a Darwin Core archive for each dataset.

```{r message=FALSE, warning=FALSE}
titles <- records %>%
  distinct(Metadata$title) %>%
  pull("Metadata$title")

titles
```

### Extracting occurrence data

Let's first create a new column for the `occurrenceID`, this will be used later to link together the measurements and occurrences.

```{r message=FALSE, warning=FALSE}
records$id <- records$Occurrence$occurrenceID
stopifnot(length(unique(records$id)) == nrow(records))
```

Here I'm creating a subset for testing purposes. At the end of the notebook we will process all subsets.

```{r message=FALSE, warning=FALSE}
records_test <- records %>%
  filter(Metadata$title == titles[1])
```

Now we can select and process the columns that will go into the occurrence table.

```{r message=FALSE, warning=FALSE}
extract_occurrences <- function(df) {
    df %>%
      select("id", "Occurrence", "Event", "Location", "Identification", "Record-level", "Taxon") %>%
      jsonlite::flatten() %>%
      rename_all(~str_replace(., ".*\\.", "")) %>%
      as_tibble()
}

occ <- extract_occurrences(records_test)
```

### Cleaning up occurrence data

First clean up any escaped newlines and fix `basisOfRecord`:

```{r message=FALSE, warning=FALSE}
occ <- occ %>%
  mutate_all(function(x) { gsub("\\n", "", x) }) %>%
  mutate(basisOfRecord = "HumanObservation")
```

Let's take a look at `scientificName` and `scientificNameID`.

```{r message=FALSE, warning=FALSE}
occ %>%
  group_by(scientificName) %>%
  summarize(records = n()) %>%
  arrange(desc(records)) %>%
  rmarkdown::paged_table()
```

```{r message=FALSE, warning=FALSE}
occ %>%
  group_by(scientificNameID) %>%
  summarize(records = n()) %>%
  arrange(desc(records)) %>%
  rmarkdown::paged_table()
```

Note that in some cases `scientificName` only contains the species epithet, this is something that should be fixed at ISA. Let's try to clean up the scientific names before we do taxon matching with WoRMS. Here I'm using the `gni_parse()` function from the `taxize` package, which connects to the [GNI](http://gni.globalnames.org/) name parser. If a name cannot be parsed, I'm keeping the original.

```{r message=FALSE, warning=FALSE}
library(taxize)
library(purrr)

names <- unique(occ$scientificName)
names <- names[names != ""]

clean_name <- function(name) {
  parsed <- tryCatch({
    res <- gni_parse(name)
    stopifnot(nrow(res) == 1)
    return(res$canonical[1])
  },
  error = function(cond){
    return(name)
  })
}

names_clean <- unlist(map(names, clean_name))
```

Now let's try taxon matching using the cleaned names.

```{r message=FALSE, warning=FALSE}
library(worrms)

match_name <- function(name) {
  lsid <- tryCatch({
    res <- wm_records_names(name)
    matches <- res[[1]] %>%
      filter(match_type == "exact" | match_type == "exact_genus")
    if (nrow(matches) > 1) {
      message(paste0("Multiple exact matches for ", name))
    }
    return(matches$lsid[1])
  }, error = function(cond) {
    return(NA)
  })
}

lsids <- unlist(map(names_clean, match_name))
```

I'll use the resulting LSIDs to replace the provided `scientificNameIDs`.

```{r message=FALSE, warning=FALSE}
names <- tibble(scientificName = names, lsid = lsids)
occ <- occ %>%
  left_join(names, by = "scientificName") %>%
  select(-scientificNameID) %>%
  rename(scientificNameID = lsid)
```

Let's take another look at the top `scientificName` and `scientificNameID` after mathing:

```{r message=FALSE, warning=FALSE}
occ %>%
  group_by(scientificName, scientificNameID) %>%
  summarize(records = n()) %>%
  arrange(desc(records)) %>%
  head(30) %>%
  knitr::kable()
```

### Extracting MeasurementOrFact data

```{r message=FALSE, warning=FALSE}
extract_mof <- function(df) {
    df %>%
      select("id", "MeasurementOrFact") %>%
      jsonlite::flatten() %>%
      rename_all(~str_replace(., ".*\\.", "")) %>%
      filter(measurementType != "") %>%
      as_tibble()
}

mof <- extract_mof(records)
mof
```

## Generating EML

As the metadata records are all the same within a dataset, let's take the first and use that to populate the EML. Note that the EML template is read from `eml.xml`:

```{r message=FALSE, warning=FALSE}
library(readr)
library(glue)

generate_eml <- function(df) {
  eml <- read_file("eml.xml")
  metadata <- df$Metadata[1,]
  firstname <- strsplit(metadata$Contact$name, " ")[[1]][1]
  lastname <- strsplit(metadata$Contact$name, " ")[[1]][2]
  organization <- metadata$Contact$organisation
  email <- metadata$Contact$email
  position <- metadata$Contact$position
  abstract <- metadata$abstract
  title <- metadata$title
  citation <- metadata$citation
  packageid <- "https://datasets.obis.org/deepdata"
  pubdate <- format(Sys.time(), "%Y-%m-%d")
  datestamp <- format(Sys.time(), "%Y-%m-%dT%H:%M:%S%z")
  glue(eml)
}

generate_eml(records_test)
```

## Generating an archive descriptor file

The archive also needs to include a `meta.xml` file which describes the files in the archive and their relationships.

Let's first get a list of terms including their `qualName`.

```{r message=FALSE, warning=FALSE}
library(xml2)

get_terms <- function(url) {
  doc <- read_xml(url)
  terms <- doc %>%
    xml_ns_strip() %>%
    xml_find_all(".//property") %>% 
    map_df(function(x) {
      list(
        name = xml_attr(x, "name"),
        qual = xml_attr(x, "qualName")
      )
    })
}

occurrence_terms <- get_terms("https://rs.gbif.org/core/dwc_occurrence_2020-07-15.xml")
mof_terms <- get_terms("https://rs.gbif.org/extension/obis/extended_measurement_or_fact.xml")
```

Using these we can generate a list of terms to go into the `meta.xml` file for each table.

```{r message=FALSE, warning=FALSE}
generate_meta <- function(occ, mof) {
  occurrence_fields <- tibble(name = names(occ)) %>%
    left_join(occurrence_terms, by = "name") %>%
    mutate(index = as.numeric(row.names(.)) - 1)
  
  occurrence_lines <- paste0("<field index=\"", occurrence_fields$index, "\" term=\"", occurrence_fields$qual, "\"/>")
  occurrence_lines[1] <- "<id index=\"0\" />"
  occurrence_lines <- paste0(occurrence_lines, collapse = "\n")

  mof_fields <- tibble(name = names(mof)) %>%
  left_join(mof_terms, by = "name") %>%
  mutate(index = as.numeric(row.names(.)) - 1)

  mof_lines <- paste0("<field index=\"", mof_fields$index, "\" term=\"", mof_fields$qual, "\"/>")
  mof_lines[1] <- "<coreid index=\"0\" />"
  mof_lines <- paste0(mof_lines, collapse = "\n")

  meta <- read_file("meta.xml")
  glue(meta)
}

generate_meta(occ, mof)
```

## Bringing it all together

```{r message=FALSE, warning=FALSE}
for (title in titles) {

  dataset <- records %>%
    filter(Metadata$title == title)

  # occurrence
    
  occ <- extract_occurrences(dataset)
  occ <- occ %>%
    mutate_all(function(x) { gsub("\\n", "", x) }) %>%
    mutate(basisOfRecord = "HumanObservation")
  names <- unique(occ$scientificName)
  names <- names[names != ""]
  names_clean <- unlist(map(names, clean_name))
  lsids <- unlist(map(names_clean, match_name))
  names <- tibble(scientificName = names, lsid = lsids)
  occ <- occ %>%
    left_join(names, by = "scientificName") %>%
    select(-scientificNameID) %>%
    rename(scientificNameID = lsid)
  
  # mof
  
  mof <- extract_mof(dataset)
  
  # EML
  
  eml <- generate_eml(dataset)
  
  # meta.xml
  
  meta <- generate_meta(occ, mof)
  
  # generate archive
  
  write.table(occ, file = "output/occurrence.txt", sep = "\t", row.names = FALSE, na = "", quote = FALSE)
  write.table(mof, file = "output/extendedmeasurementorfact.txt", sep = "\t", row.names = FALSE, na = "", quote = FALSE)
  writeLines(eml, "output/eml.xml")
  writeLines(meta, "output/meta.xml")
  
  files <- c("occurrence.txt", "extendedmeasurementorfact.txt", "eml.xml", "meta.xml")
  setwd("output")
  file.remove(glue("{title}.zip"))
  zip(glue("{title}.zip"), files)
  for (f in files) {
    file.remove(f)
  }
  setwd("..")

}
```
