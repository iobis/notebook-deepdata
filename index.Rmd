---
title: ISA DeepData
date: "`r Sys.Date()`"
author: Pieter Provoost
output: (function(...) {
  rmdformats::robobook(toc_depth = 3, ...) })
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "docs") })  
---

This notebook processes data from the [International Seabed Authority](https://www.isa.org.jm/) (ISA) [DeepData](https://data.isa.org.jm/isa/map/) database into Darwin Core archives. The resulting datasets are hosted at [https://datasets.obis.org/hosted/isa/index.html](https://datasets.obis.org/hosted/isa/index.html).

The code for this notebook is hosted at https://github.com/iobis/notebook-deepdata.

## Data flow

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(DiagrammeR)

grViz("
digraph boxes_and_circles {
  graph [overlap = true, fontsize = 10]
  node [shape = box, fontname = Helvetica, fillcolor = Gray95, style = filled]
  ISA [label = 'ISA FTP'] ; notebook [label = 'R notebook'] ; S3 [label = 'S3 storage'] ; OBIS [label = 'OBIS harvester']
  node [shape = circle, fontname = Helvetica]
  a [label = 'dataset'] ; b [label = 'dataset'] ; c [label = 'dataset'] ; d [label = 'dataset']
  subgraph cluster1 {
    node [fixedsize = true, width = 3]
    S3->{a b c d}
  }
  ISA->notebook notebook->S3 S3->OBIS
}
", width = 600)
```

## Fetching the data from the ISA server

The DeepData dataset is delivered to OBIS via FTP. Let's list the files on the FTP server and download the most recent JSON file to a temporary directory. Credentials are stored in `env.txt`.

```{r message=FALSE, warning=FALSE}
require(RCurl)
library(stringr)
library(dplyr)

readRenviron("env.txt")
url <- Sys.getenv("deepdata_url")
res <- getURL(url, verbose = TRUE, ftp.use.epsv = FALSE, dirlistonly = TRUE)

filenames <- unlist(strsplit(res, "\\n"))
filenames <- sort(filenames[str_detect(filenames, ".json")], decreasing = TRUE)

if (length(filenames) != 1) {
  stop("Unexpected number of files found")
}

file_url <- paste0(url, "/", filenames[1])
temp_file <- tempfile(pattern = "deepdata_", tmpdir = tempdir(), fileext = ".json")
download.file(file_url, temp_file)
```

## Parsing the JSON file

Earlier versions of the file were encoded in the non-standard `ISO-8859-1`, requiring the need to use `readLines` before parsing the data with the jsonlite package, but that seems to be fixed now.

```{r message=FALSE, warning=FALSE}
library(jsonlite)
library(purrr)

con <- file(temp_file)
lines <- readLines(file(temp_file, encoding = "UTF-8"), warn = FALSE)
close(con)
records <- fromJSON(lines, simplifyDataFrame = TRUE)$DEEPDATA %>%
  as_tibble()
```

## Generating Darwin Core data files

We can now extract a list of distinct datasets from the data frame, and generate a Darwin Core archive for each dataset. I'm using `group_indices()` to assign a unique dataset identifier to each record.

```{r message=FALSE, warning=FALSE}
dataset_ids <- records %>%
  group_indices(Metadata)
records$dataset_id <- dataset_ids  
```

Let's also take a look at some titles and abstracts.

```{r message=FALSE, warning=FALSE}
titles <- records %>%
  distinct(Metadata$title) %>%
  pull("Metadata$title")

titles
```

```{r message=FALSE, warning=FALSE}
abstracts <- records %>%
  distinct(Metadata$abstract) %>%
  pull("Metadata$abstract")

abstracts
```

### Extracting occurrence data

Let's first create a new ID column, this will be used later to link together the measurements and occurrences, and to select records by dataset. We cannot use `occurrenceID` here because these are not unique within the dataset.

```{r message=FALSE, warning=FALSE}
library(uuid)

records$id <- UUIDgenerate(use.time = NA, n = nrow(records))
stopifnot(length(unique(records$id)) == nrow(records))
```

Now we can select and process the columns that will go into the occurrence table.

```{r message=FALSE, warning=FALSE}
extract_occurrences <- function(df) {
    df %>%
      select("id", "dataset_id", "Occurrence", "Event", "Location", "Identification", "Record-level", "Taxon") %>%
      jsonlite::flatten() %>%
      rename_all(~str_replace(., ".*\\.", "")) %>%
      as_tibble()
}

occ <- extract_occurrences(records)
```

### Initial cleanup of occurrence data

First clean up any escaped newlines, empty strings, and placeholder values. Also fix `basisOfRecord`:

```{r message=FALSE, warning=FALSE}
library(stringr)

occ <- occ %>%
  mutate_all(~gsub("\\n", "", .)) %>%
  mutate_all(~na_if(., "")) %>%
  mutate(across(where(is.character), str_squish)) %>%
  mutate_all(~replace(., . %in% c("indet", "Not Reported", "indet."), NA)) %>%
  mutate(basisOfRecord = "HumanObservation")
```

Let's take a look at `scientificName` and `scientificNameID`.

```{r message=FALSE, warning=FALSE}
occ %>%
  group_by(scientificName) %>%
  summarize(records = n()) %>%
  arrange(desc(records)) %>%
  rmarkdown::paged_table()
```

```{r message=FALSE, warning=FALSE}
occ %>%
  group_by(scientificNameID) %>%
  summarize(records = n()) %>%
  arrange(desc(records)) %>%
  rmarkdown::paged_table()
```

So at least in the current version at the time of writing (May 2021) there are some quality issues for both fields.

### Fixing taxonomy

Let's try to clean up the scientific names before we do taxon matching with WoRMS. Here I'm using the `gni_parse()` function from the `taxize` package, which connects to the [GNI](http://gni.globalnames.org/) name parser. If a name cannot be parsed, I'm keeping the original.

The first step is to create a list of all distinct names in the taxonomy columns.

```{r message=FALSE, warning=FALSE}
taxonomy <- occ %>%
  select(phylum, class, order, family, genus, scientificName)
names <- na.omit(unique(unlist(taxonomy)))
```

Then pass through the name parser:

```{r message=FALSE, warning=FALSE, cache=TRUE}
library(taxize)

clean_name <- function(name) {
  parsed <- tryCatch({
    res <- gni_parse(name)
    stopifnot(nrow(res) == 1)
    return(res$canonical[1])
  },
  error = function(cond){
    return(name)
  })
}

names_clean <- sapply(names, clean_name)  
```

Now use the cleaned names for taxon matching:

```{r message=FALSE, warning=FALSE, cache=TRUE}
library(worrms)

match_name <- function(name) {
  lsid <- tryCatch({
    res <- wm_records_names(name)
    matches <- res[[1]] %>%
      filter(match_type == "exact" | match_type == "exact_genus")
    if (nrow(matches) > 1) {
      message(paste0("Multiple exact matches for ", name))
    }
    return(matches$lsid[1])
  }, error = function(cond) {
    return(NA)
  })
}

lsids <- sapply(names_clean, match_name)
```

Now we need to find the lowest taxonomic level at which we find a name. Note that this will result in records with less taxonomic resolution than intended. Ideally we would only match on `scientificName`. First translate the taxonomy columns to LSIDs:

```{r message=FALSE, warning=FALSE}
taxonomy_clean <- taxonomy %>%
  mutate_all(~names_clean[.]) %>%
  mutate_all(~lsids[.])

taxonomy_clean
```

The find the most specific one for each row:

```{r message=FALSE, warning=FALSE}
taxonomy_clean <- taxonomy_clean %>%
  mutate(best = coalesce(scientificName, genus, family, order, class))
```

I'll use the resulting LSIDs to replace the provided `scientificNameIDs`.

```{r message=FALSE, warning=FALSE}
occ$scientificNameID <- taxonomy_clean$best
```

Let's take another look at the top `scientificName` and `scientificNameID` after mathing:

```{r message=FALSE, warning=FALSE}
occ %>%
  group_by(scientificName, scientificNameID) %>%
  summarize(records = n()) %>%
  arrange(desc(records)) %>%
  head(30) %>%
  knitr::kable()
```

### Extracting MeasurementOrFact data

```{r message=FALSE, warning=FALSE}
extract_mof <- function(df) {
    df %>%
      select("id", "dataset_id", "MeasurementOrFact") %>%
      jsonlite::flatten() %>%
      rename_all(~str_replace(., ".*\\.", "")) %>%
      mutate(across(where(is.character), str_squish)) %>%
      mutate_all(~na_if(., "")) %>%
      filter(!is.na(measurementType) & !is.na(measurementValue)) %>%
      as_tibble()
}

mof <- extract_mof(records)
mof
```

A large number of records appear to have empty values. To demonstrate this, let's take a look at the most common combinations of `measurementType` and `measurementValue`:

```{r message=FALSE, warning=FALSE}
mof %>%
  group_by(measurementType, measurementValue) %>%
  summarize(records = n()) %>%
  arrange(desc(records)) %>%
  head(10) %>%
  knitr::kable()
```
## Generating Darwin Core Archives

### Generating EML

For demonstration purposes, I'm working with the dataset pertaining to the first record here. The EML template is read from `templates/eml.xml`:

```{r message=FALSE, warning=FALSE}
library(readr)
library(glue)

generate_eml <- function(df) {
  eml <- read_file("templates/eml.xml")
  metadata <- df$Metadata[1,]
  firstname <- strsplit(metadata$Contact$name, " ")[[1]][1]
  lastname <- strsplit(metadata$Contact$name, " ")[[1]][2]
  organization <- metadata$Contact$organisation
  email <- metadata$Contact$email
  position <- metadata$Contact$position
  abstract <- metadata$abstract
  title <- metadata$title
  citation <- metadata$citation
  packageid <- "https://datasets.obis.org/deepdata"
  pubdate <- format(Sys.time(), "%Y-%m-%d")
  datestamp <- format(Sys.time(), "%Y-%m-%dT%H:%M:%S%z")
  glue(eml)
}

generate_eml(records)
```

### Generating an archive descriptor file

The archive also needs to include a `meta.xml` file which describes the files in the archive and their relationships.

Let's first get a list of terms including their `qualName`.

```{r message=FALSE, warning=FALSE}
library(xml2)

get_terms <- function(url) {
  doc <- read_xml(url)
  terms <- doc %>%
    xml_ns_strip() %>%
    xml_find_all(".//property") %>% 
    map_df(function(x) {
      list(
        name = xml_attr(x, "name"),
        qual = xml_attr(x, "qualName")
      )
    })
}

occurrence_terms <- get_terms("https://rs.gbif.org/core/dwc_occurrence_2020-07-15.xml")
mof_terms <- get_terms("https://rs.gbif.org/extension/obis/extended_measurement_or_fact.xml")
```

Using these we can generate a list of terms to go into the `meta.xml` file for each table.

```{r message=FALSE, warning=FALSE}
generate_meta <- function(occ, mof) {
  occurrence_fields <- tibble(name = names(occ)) %>%
    left_join(occurrence_terms, by = "name") %>%
    mutate(index = as.numeric(row.names(.)) - 1)
  
  occurrence_lines <- paste0("<field index=\"", occurrence_fields$index, "\" term=\"", occurrence_fields$qual, "\"/>")
  occurrence_lines[1] <- "<id index=\"0\" />"
  occurrence_lines <- paste0(occurrence_lines, collapse = "\n")

  mof_fields <- tibble(name = names(mof)) %>%
  left_join(mof_terms, by = "name") %>%
  mutate(index = as.numeric(row.names(.)) - 1)

  mof_lines <- paste0("<field index=\"", mof_fields$index, "\" term=\"", mof_fields$qual, "\"/>")
  mof_lines[1] <- "<coreid index=\"0\" />"
  mof_lines <- paste0(mof_lines, collapse = "\n")

  meta <- read_file("templates/meta.xml")
  glue(meta)
}

generate_meta(occ, mof)
```

### Bringing it all together

Now we can generate an archive for each dataset. For now I'm adding the dataset ID to the title because there's only a single title within the entire dataset.

While I'm generating datasets I'm also populating the RSS feed and creating dataset landing pages.

```{r message=FALSE, warning=FALSE}
baseurl <- "https://datasets.obis.org/hosted/isa/"
item_template <- read_file("templates/rss_item.xml")
landing_template <- read_file("templates/index_dataset.html")
items <- list()
shortnames <- list()
pubdate <- format(Sys.time(), "%a, %d %b %Y %H:%M:%S %z")

for (datasetid in unique(records$dataset_id)) {
  
  dataset <- records %>%
    filter(dataset_id == datasetid) %>%
    head(1)
  
  title <- paste0(dataset$Metadata$title, "_", datasetid)
  abstract <- dataset$Metadata$abstract
  shortname <- str_replace(tolower(title), "\\s", "_")
  link <- paste0(baseurl, shortname, "/index.html")
  dwca <- paste0(baseurl, shortname, "/", shortname, ".zip")

  # clear dataset directory
    
  unlink(paste0("output/", shortname), recursive = TRUE)
  dir.create(paste0("output/", shortname))

  # RSS feed items
  
  item <- glue(item_template)
  items[[datasetid]] <- item
  
  # shortnames for the ISA landing page
  
  shortnames[[datasetid]] <- shortname

  # dataset landing page
  
  landing <- glue(landing_template)
  writeLines(landing, paste0("output/", shortname, "/index.html"))
  
  # archive  
  
  dataset_occ <- occ %>% filter(dataset_id == datasetid) 
  dataset_mof <- mof %>% filter(dataset_id == datasetid) 

  eml <- generate_eml(dataset)
  meta <- generate_meta(occ, mof)
  
  write.table(dataset_occ, file = paste0("output/", shortname, "/occurrence.txt"), sep = "\t", row.names = FALSE, na = "", quote = FALSE)
  write.table(dataset_mof, file = paste0("output/", shortname, "/extendedmeasurementorfact.txt"), sep = "\t", row.names = FALSE, na = "", quote = FALSE)
  writeLines(eml, paste0("output/", shortname, "/eml.xml"))
  writeLines(meta, paste0("output/", shortname, "/meta.xml"))
  
  files <- c("occurrence.txt", "extendedmeasurementorfact.txt", "eml.xml", "meta.xml")
  setwd(paste0("output/", shortname))
  zip(glue("{shortname}.zip"), files)
  for (f in files) {
    file.remove(f)
  }
  setwd("../..")

}
```

## Data publishing

In this section all files are uploaded to an S3 bucket. A list of datasets is visible at [https://datasets.obis.org/hosted/isa/index.html](https://datasets.obis.org/hosted/isa/index.html), and an [RSS file](https://datasets.obis.org/hosted/isa/rss.xml) is available for the OBIS harvester.

### Generate RSS file

```{r message=FALSE, warning=FALSE}
items <- paste0(items, collapse = "\n")
rss_template <- read_file("templates/rss.xml")

title <- "International Seabed Authority (ISA)"
description <- "International Seabed Authority (ISA)"
link <- paste0(baseurl, "index.html")

rss <- glue(rss_template)
writeLines(rss, "output/rss.xml")
```

### Generate landing page

```{r message=FALSE, warning=FALSE}
index_template <- read_file("templates/index.html")
content <- paste0(paste0("<li><a href=\"", shortnames, "/index.html\">", shortnames, "</a></li>"), collapse = "\n")
index <- glue(index_template)
writeLines(index, "output/index.html")
```

### Uploading to S3

```{r message=FALSE, warning=FALSE, results='hide'}
library("aws.s3")

delete_object("hosted/isa/", bucket = "obis-datasets")
files <- list.files("output", full.names = TRUE, recursive = TRUE, include.dirs = FALSE)

for (file in files) {
  folder <- str_replace(dirname(file), "output", "hosted/isa")
  target <- str_replace(file, "output", "hosted/isa")
  message(target)
  put_object(file, object = target, bucket = "obis-datasets", acl = "public-read")
}
```

